{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DPIFaKEPQayC"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess train"
      ],
      "metadata": {
        "id": "hREfUfrOQW_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_fWULNyjYg0x"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\", device=device)\n",
        "rubert_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\").to(device)"
      ],
      "metadata": {
        "id": "3vUFHVODvb2f"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = pd.read_csv('https://raw.githubusercontent.com/PhilBurub/NLPcourse_HSE/main/train_aspects.txt', header=None, delimiter='\\t')"
      ],
      "metadata": {
        "id": "9g3o06DyZRvz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = pd.read_csv('https://raw.githubusercontent.com/PhilBurub/NLPcourse_HSE/main/train_reviews.txt', header=None, delimiter='\\t', index_col=0)"
      ],
      "metadata": {
        "id": "V1xlW69DZT3O"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boudaries(text):\n",
        "  out = []\n",
        "  new_text = text\n",
        "  cur = 0\n",
        "  for word in tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])[1:-1]:\n",
        "    new_word = word.lstrip('##')\n",
        "    start = new_text.find(new_word)\n",
        "    end = start + len(new_word)\n",
        "    out.append((word, cur + start, cur + end, 'O'))\n",
        "    cur += end\n",
        "    new_text = new_text[end:]\n",
        "  return pd.DataFrame(out)"
      ],
      "metadata": {
        "id": "0DbzWZ1cfxca"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "  pass\n",
        "\n",
        "for review in ans[0].unique():\n",
        "  anno = ans[ans[0] == review]\n",
        "  out = boudaries(texts.loc[review][1])\n",
        "  for _, row in anno.iterrows():\n",
        "    start = True\n",
        "    for idx, _ in out[(out[1] >= row[3]) & (out[2] <= row[4])].sort_values(1).iterrows():\n",
        "      out.iloc[idx, 3] = f'B-{row[1]}' if start else f'I-{row[1]}'\n",
        "      start = False\n",
        "\n",
        "  with open('train.txt', 'a', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(map(lambda x: '\\t'.join(x), out[[0, 3]].values)) + '\\n\\n')"
      ],
      "metadata": {
        "id": "_NYsks1Nif-W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model\n",
        "\n"
      ],
      "metadata": {
        "id": "DPIFaKEPQayC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def read_corpus(filepath):\n",
        "    \"\"\" Read corpus from the given file path.\n",
        "    Args:\n",
        "        filepath: file path of the corpus\n",
        "    Returns:\n",
        "        sentences: a list of sentences, each sentence is a list of str\n",
        "        tags: corresponding tags\n",
        "    \"\"\"\n",
        "    sentences, tags = [], []\n",
        "    sent, tag = ['<START>'], ['<START>']\n",
        "    with open(filepath, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            if line == '\\n':\n",
        "                if len(sent) > 1:\n",
        "                    sentences.append(sent + ['<END>'])\n",
        "                    tags.append(tag + ['<END>'])\n",
        "                sent, tag = ['<START>'], ['<START>']\n",
        "            else:\n",
        "                line = line.split()\n",
        "                sent.append(line[0])\n",
        "                tag.append(line[1])\n",
        "    return sentences, tags\n",
        "\n",
        "\n",
        "def generate_train_dev_dataset(filepath, sent_vocab, tag_vocab, train_proportion=0.8):\n",
        "    \"\"\" Read corpus from given file path and split it into train and dev parts\n",
        "    Args:\n",
        "        filepath: file path\n",
        "        sent_vocab: sentence vocab\n",
        "        tag_vocab: tag vocab\n",
        "        train_proportion: proportion of training data\n",
        "    Returns:\n",
        "        train_data: data for training, list of tuples, each containing a sentence and corresponding tag.\n",
        "        dev_data: data for development, list of tuples, each containing a sentence and corresponding tag.\n",
        "    \"\"\"\n",
        "    sentences, tags = read_corpus(filepath)\n",
        "    tags = words2indices(tags, tag_vocab)\n",
        "    data = list(zip(sentences, tags))\n",
        "    random.shuffle(data)\n",
        "    n_train = int(len(data) * train_proportion)\n",
        "    train_data, dev_data = data[: n_train], data[n_train:]\n",
        "    return train_data, dev_data\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size=32, shuffle=True):\n",
        "    \"\"\" Yield batch of (sent, tag), by the reversed order of source length.\n",
        "    Args:\n",
        "        data: list of tuples, each tuple contains a sentence and corresponding tag.\n",
        "        batch_size: batch size\n",
        "        shuffle: bool value, whether to random shuffle the data\n",
        "    \"\"\"\n",
        "    data_size = len(data)\n",
        "    indices = list(range(data_size))\n",
        "    if shuffle:\n",
        "        random.shuffle(indices)\n",
        "    batch_num = (data_size + batch_size - 1) // batch_size\n",
        "    for i in range(batch_num):\n",
        "        batch = [data[idx] for idx in indices[i * batch_size: (i + 1) * batch_size]]\n",
        "        batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "        sentences = [x[0] for x in batch]\n",
        "        tags = [x[1] for x in batch]\n",
        "        yield sentences, tags\n",
        "\n",
        "\n",
        "def words2indices(origin, vocab):\n",
        "    \"\"\" Transform a sentence or a list of sentences from str to int\n",
        "    Args:\n",
        "        origin: a sentence of type list[str], or a list of sentences of type list[list[str]]\n",
        "        vocab: Vocab instance\n",
        "    Returns:\n",
        "        a sentence or a list of sentences represented with int\n",
        "    \"\"\"\n",
        "    if isinstance(origin[0], list):\n",
        "        result = [[vocab[w] for w in sent] for sent in origin]\n",
        "    else:\n",
        "        result = [vocab[w] for w in origin]\n",
        "    return result\n",
        "\n",
        "\n",
        "def indices2words(origin, vocab):\n",
        "    \"\"\" Transform a sentence or a list of sentences from int to str\n",
        "    Args:\n",
        "        origin: a sentence of type list[int], or a list of sentences of type list[list[int]]\n",
        "        vocab: Vocab instance\n",
        "    Returns:\n",
        "        a sentence or a list of sentences represented with str\n",
        "    \"\"\"\n",
        "    if isinstance(origin[0], list):\n",
        "        result = [[vocab.id2word(w) for w in sent] for sent in origin]\n",
        "    else:\n",
        "        result = [vocab.id2word(w) for w in origin]\n",
        "    return result\n",
        "\n",
        "\n",
        "def pad(data, padded_token, device):\n",
        "    \"\"\" pad data so that each sentence has the same length as the longest sentence\n",
        "    Args:\n",
        "        data: list of sentences, List[List[word]]\n",
        "        padded_token: padded token\n",
        "        device: device to store data\n",
        "    Returns:\n",
        "        padded_data: padded data, a tensor of shape (max_len, b)\n",
        "        lengths: lengths of batches, a list of length b.\n",
        "    \"\"\"\n",
        "    lengths = [len(sent) for sent in data]\n",
        "    max_len = lengths[0]\n",
        "    padded_data = []\n",
        "    for s in data:\n",
        "        padded_data.append(s + [padded_token] * (max_len - len(s)))\n",
        "    return torch.tensor(padded_data, device=device), lengths\n",
        "\n",
        "\n",
        "def print_var(**kwargs):\n",
        "    for k, v in kwargs.items():\n",
        "        print(k, v)\n",
        "\n",
        "\n",
        "def main():\n",
        "    sentences, tags = read_corpus('train.txt')\n",
        "    print(len(sentences), len(tags))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDoQC02KWAFw",
        "outputId": "c653254e-b5f3-4706-9b1d-3bd4fedad8a2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "284 284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Usage:\n",
        "    vocab.py TRAIN SENT_VOCAB TAG_VOCAB [options]\n",
        "\n",
        "Options:\n",
        "    --max-size=<int>   maximum size of the dictionary [default: 5000]\n",
        "    --freq-cutoff=<int>     frequency cutoff [default: 2]\n",
        "\"\"\"\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, word2id, id2word):\n",
        "        self.UNK = '<UNK>'\n",
        "        self.PAD = '<PAD>'\n",
        "        self.START = '<START>'\n",
        "        self.END = '<END>'\n",
        "        self.__word2id = word2id\n",
        "        self.__id2word = id2word\n",
        "\n",
        "    def get_word2id(self):\n",
        "        return self.__word2id\n",
        "\n",
        "    def get_id2word(self):\n",
        "        return self.__id2word\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if self.UNK in self.__word2id:\n",
        "            return self.__word2id.get(item, self.__word2id[self.UNK])\n",
        "        return self.__word2id[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__word2id)\n",
        "\n",
        "    def id2word(self, idx):\n",
        "        return self.__id2word[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def build(data, max_dict_size, freq_cutoff, is_tags):\n",
        "        \"\"\" Build vocab from the given data\n",
        "        Args:\n",
        "            data (List[List[str]]): List of sentences, each sentence is a list of str\n",
        "            max_dict_size (int): The maximum size of dict\n",
        "                                 If the number of valid words exceeds dict_size, only the most frequently-occurred\n",
        "                                 max_dict_size words will be kept.\n",
        "            freq_cutoff (int): If a word occurs less than freq_size times, it will be dropped.\n",
        "            is_tags (bool): whether this Vocab is for tags\n",
        "        Returns:\n",
        "            vocab: The Vocab instance generated from the given data\n",
        "        \"\"\"\n",
        "        word_counts = Counter(chain(*data))\n",
        "        valid_words = [w for w, d in word_counts.items() if d >= freq_cutoff]\n",
        "        valid_words = sorted(valid_words, key=lambda x: word_counts[x], reverse=True)\n",
        "        valid_words = valid_words[: max_dict_size]\n",
        "        valid_words += ['<PAD>']\n",
        "        word2id = {w: idx for idx, w in enumerate(valid_words)}\n",
        "        if not is_tags:\n",
        "            word2id['<UNK>'] = len(word2id)\n",
        "            valid_words += ['<UNK>']\n",
        "        return Vocab(word2id=word2id, id2word=valid_words)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        with open(file_path, 'w', encoding='utf8') as f:\n",
        "            json.dump({'word2id': self.__word2id, 'id2word': self.__id2word}, f, ensure_ascii=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        with open(file_path, 'r', encoding='utf8') as f:\n",
        "            entry = json.load(f)\n",
        "        return Vocab(word2id=entry['word2id'], id2word=entry['id2word'])\n",
        "\n",
        "\n",
        "def main():\n",
        "    sentences, tags = read_corpus('train.txt')\n",
        "    sent_vocab = Vocab.build(sentences, 10**5, 3, is_tags=False)\n",
        "    tag_vocab = Vocab.build(tags, 10**5, 0, is_tags=True)\n",
        "    sent_vocab.save('sent_vocab.json')\n",
        "    tag_vocab.save('tag_vocab.json')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "VT9ewsDvWQgK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cJxY7hEsVyW4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class BiLSTMCRF(nn.Module):\n",
        "    def __init__(self, sent_vocab, tag_vocab, dropout_rate=0.5, embed_size=256, hidden_size=256):\n",
        "        \"\"\" Initialize the model\n",
        "        Args:\n",
        "            sent_vocab (Vocab): vocabulary of words\n",
        "            tag_vocab (Vocab): vocabulary of tags\n",
        "            embed_size (int): embedding size\n",
        "            hidden_size (int): hidden state size\n",
        "        \"\"\"\n",
        "        super(BiLSTMCRF, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sent_vocab = sent_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "        self.embedding = nn.Linear(312, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, bidirectional=True)\n",
        "        self.hidden2emit_score = nn.Linear(hidden_size * 2, len(self.tag_vocab))\n",
        "        self.transition = nn.Parameter(torch.randn(len(self.tag_vocab), len(self.tag_vocab)))  # shape: (K, K)\n",
        "\n",
        "    def forward(self, sentences, mask, tags, sen_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentences (tensor): sentences, shape (b, len). Lengths are in decreasing order, len is the length\n",
        "                                of the longest sentence\n",
        "            tags (tensor): corresponding tags, shape (b, len)\n",
        "            sen_lengths (list): sentence lengths\n",
        "        Returns:\n",
        "            loss (tensor): loss on the batch, shape (b,)\n",
        "        \"\"\"\n",
        "        sentences = sentences  # shape: (len, b)\n",
        "        sentences = self.embedding(sentences).transpose(0, 1)  # shape: (len, b, e)\n",
        "        emit_score = self.encode(sentences, sen_lengths)  # shape: (b, len, K)\n",
        "        loss = self.cal_loss(tags, mask, emit_score.transpose(0, 1))  # shape: (b,)\n",
        "        return loss\n",
        "\n",
        "    def encode(self, sentences, sent_lengths):\n",
        "        \"\"\" BiLSTM Encoder\n",
        "        Args:\n",
        "            sentences (tensor): sentences with word embeddings, shape (len, b, e)\n",
        "            sent_lengths (list): sentence lengths\n",
        "        Returns:\n",
        "            emit_score (tensor): emit score, shape (b, len, K)\n",
        "        \"\"\"\n",
        "        hidden_states, _ = self.encoder(sentences)\n",
        "        #hidden_states, _ = pad_packed_sequence(hidden_states, batch_first=True)  # shape: (b, len, 2h)\n",
        "        emit_score = self.hidden2emit_score(hidden_states)  # shape: (b, len, K)\n",
        "        emit_score = self.dropout(emit_score)  # shape: (b, len, K)\n",
        "        return emit_score\n",
        "\n",
        "    def cal_loss(self, tags, mask, emit_score):\n",
        "        \"\"\" Calculate CRF loss\n",
        "        Args:\n",
        "            tags (tensor): a batch of tags, shape (b, len)\n",
        "            mask (tensor): mask for the tags, shape (b, len), values in PAD position is 0\n",
        "            emit_score (tensor): emit matrix, shape (b, len, K)\n",
        "        Returns:\n",
        "            loss (tensor): loss of the batch, shape (b,)\n",
        "        \"\"\"\n",
        "        batch_size, sent_len = tags.shape\n",
        "        # calculate score for the tags\n",
        "        score = torch.gather(emit_score, dim=2, index=tags.unsqueeze(dim=2)).squeeze(dim=2)  # shape: (b, len)\n",
        "        score[:, 1:] += self.transition[tags[:, :-1], tags[:, 1:]]\n",
        "        total_score = (score * mask.type(torch.float)).sum(dim=1)  # shape: (b,)\n",
        "        # calculate the scaling factor\n",
        "        d = torch.unsqueeze(emit_score[:, 0], dim=1)  # shape: (b, 1, K)\n",
        "        for i in range(1, sent_len):\n",
        "            n_unfinished = mask[:, i].sum()\n",
        "            d_uf = d[: n_unfinished]  # shape: (uf, 1, K)\n",
        "            emit_and_transition = emit_score[: n_unfinished, i].unsqueeze(dim=1) + self.transition  # shape: (uf, K, K)\n",
        "            log_sum = d_uf.transpose(1, 2) + emit_and_transition  # shape: (uf, K, K)\n",
        "            max_v = log_sum.max(dim=1)[0].unsqueeze(dim=1)  # shape: (uf, 1, K)\n",
        "            log_sum = log_sum - max_v  # shape: (uf, K, K)\n",
        "            d_uf = max_v + torch.logsumexp(log_sum, dim=1).unsqueeze(dim=1)  # shape: (uf, 1, K)\n",
        "            d = torch.cat((d_uf, d[n_unfinished:]), dim=0)\n",
        "        d = d.squeeze(dim=1)  # shape: (b, K)\n",
        "        max_d = d.max(dim=-1)[0]  # shape: (b,)\n",
        "        d = max_d + torch.logsumexp(d - max_d.unsqueeze(dim=1), dim=1)  # shape: (b,)\n",
        "        llk = total_score - d  # shape: (b,)\n",
        "        loss = -llk  # shape: (b,)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, mask, sen_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentences (tensor): sentences, shape (b, len). Lengths are in decreasing order, len is the length\n",
        "                                of the longest sentence\n",
        "            sen_lengths (list): sentence lengths\n",
        "        Returns:\n",
        "            tags (list[list[str]]): predicted tags for the batch\n",
        "        \"\"\"\n",
        "        batch_size = sentences.shape[0]\n",
        "        sentences = sentences.transpose(0, 1)  # shape: (len, b)\n",
        "        sentences = self.embedding(sentences)  # shape: (len, b, e)\n",
        "        emit_score = self.encode(sentences, sen_lengths).transpose(1, 0)  # shape: (b, len, K)\n",
        "        tags = [[[i] for i in range(len(self.tag_vocab))]] * batch_size  # list, shape: (b, K, 1)\n",
        "        d = torch.unsqueeze(emit_score[:, 0], dim=1)  # shape: (b, 1, K)\n",
        "        for i in range(1, max(sen_lengths)):\n",
        "            n_unfinished = mask[:, i].sum()\n",
        "            d_uf = d[: n_unfinished]  # shape: (uf, 1, K)\n",
        "            emit_and_transition = self.transition + emit_score[: n_unfinished, i].unsqueeze(dim=1)  # shape: (uf, K, K)\n",
        "            new_d_uf = d_uf.transpose(1, 2) + emit_and_transition  # shape: (uf, K, K)\n",
        "            d_uf, max_idx = torch.max(new_d_uf, dim=1)\n",
        "            max_idx = max_idx.tolist()  # list, shape: (nf, K)\n",
        "            tags[: n_unfinished] = [[tags[b][k] + [j] for j, k in enumerate(max_idx[b])] for b in range(n_unfinished)]\n",
        "            d = torch.cat((torch.unsqueeze(d_uf, dim=1), d[n_unfinished:]), dim=0)  # shape: (b, 1, K)\n",
        "        d = d.squeeze(dim=1)  # shape: (b, K)\n",
        "        _, max_idx = torch.max(d, dim=1)  # shape: (b,)\n",
        "        max_idx = max_idx.tolist()\n",
        "        tags = [tags[b][k] for b, k in enumerate(max_idx)]\n",
        "        return tags\n",
        "\n",
        "    def save(self, filepath):\n",
        "        params = {\n",
        "            'sent_vocab': self.sent_vocab,\n",
        "            'tag_vocab': self.tag_vocab,\n",
        "            'args': dict(dropout_rate=self.dropout_rate, embed_size=self.embed_size, hidden_size=self.hidden_size),\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "        torch.save(params, filepath)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filepath, device_to_load):\n",
        "        params = torch.load(filepath, map_location=lambda storage, loc: storage)\n",
        "        model = BiLSTMCRF(params['sent_vocab'], params['tag_vocab'], **params['args'])\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "        model.to(device_to_load)\n",
        "        return model\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.embedding.weight.device\n",
        "\n",
        "\n",
        "def main():\n",
        "    sent_vocab = Vocab.load('sent_vocab.json')\n",
        "    tag_vocab = Vocab.load('tag_vocab.json')\n",
        "    train_data, dev_data = generate_train_dev_dataset('train.txt', sent_vocab, tag_vocab)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = BiLSTMCRF(sent_vocab, tag_vocab)\n",
        "    model.to(device)\n",
        "    model.save('model.pth')\n",
        "    model = model.load('model.pth', device)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ksxKsGlMQrqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Usage:\n",
        "    run.py train TRAIN SENT_VOCAB TAG_VOCAB [options]\n",
        "    run.py test TEST RESULT SENT_VOCAB TAG_VOCAB MODEL [options]\n",
        "\n",
        "Options:\n",
        "    --dropout-rate=<float>              dropout rate [default: 0.5]\n",
        "    --embed-size=<int>                  size of word embedding [default: 256]\n",
        "    --hidden-size=<int>                 size of hidden state [default: 256]\n",
        "    --batch-size=<int>                  batch-size [default: 32]\n",
        "    --max-epoch=<int>                   max epoch [default: 10]\n",
        "    --clip_max_norm=<float>             clip max norm [default: 5.0]\n",
        "    --lr=<float>                        learning rate [default: 0.001]\n",
        "    --log-every=<int>                   log every [default: 10]\n",
        "    --validation-every=<int>            validation every [default: 250]\n",
        "    --patience-threshold=<float>        patience threshold [default: 0.98]\n",
        "    --max-patience=<int>                time of continuous worse performance to decay lr [default: 4]\n",
        "    --max-decay=<int>                   time of lr decay to early stop [default: 4]\n",
        "    --lr-decay=<float>                  decay rate of lr [default: 0.5]\n",
        "    --model-save-path=<file>            model save path [default: ./model/model.pth]\n",
        "    --optimizer-save-path=<file>        optimizer save path [default: ./model/optimizer.pth]\n",
        "    --cuda                              use GPU\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "\n",
        "def train():\n",
        "    \"\"\" Training BiLSTMCRF model\n",
        "    Args:\n",
        "        args: dict that contains options in command\n",
        "    \"\"\"\n",
        "    sent_vocab = Vocab.load('sent_vocab.json')\n",
        "    tag_vocab = Vocab.load('tag_vocab.json')\n",
        "    train_data, dev_data = generate_train_dev_dataset('train.txt', sent_vocab, tag_vocab)\n",
        "    print('num of training examples: %d' % (len(train_data)))\n",
        "    print('num of development examples: %d' % (len(dev_data)))\n",
        "\n",
        "    max_epoch = 1000\n",
        "    log_every = 5\n",
        "    validation_every = 10\n",
        "    model_save_path = 'model.pth'\n",
        "    optimizer_save_path = 'opt.pth'\n",
        "    min_dev_loss = float('inf')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    patience, decay_num = 0, 0\n",
        "\n",
        "    model = BiLSTMCRF(sent_vocab, tag_vocab, 0.2, 256, 256).to(device)\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, 0, 0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    train_iter = 0  # train iter num\n",
        "    record_loss_sum, record_tgt_word_sum, record_batch_size = 0, 0, 0  # sum in one training log\n",
        "    cum_loss_sum, cum_tgt_word_sum, cum_batch_size = 0, 0, 0  # sum in one validation log\n",
        "    record_start, cum_start = time.time(), time.time()\n",
        "\n",
        "    print('start training...')\n",
        "    for epoch in range(max_epoch):\n",
        "        for sentences, tags in batch_iter(train_data, batch_size=256):\n",
        "            train_iter += 1\n",
        "            current_batch_size = len(sentences)\n",
        "            sentences_text = [tokenizer.convert_tokens_to_string(sent[1:-1]) for sent in sentences]\n",
        "            tokenized = tokenizer(sentences_text, return_tensors='pt',\n",
        "                                  padding=True).to(device)\n",
        "            embded = rubert_model(**tokenized)['last_hidden_state'].to(device)\n",
        "            mask = tokenized['attention_mask'].to(device)\n",
        "            sent_lengths = mask.sum(1)\n",
        "\n",
        "            tags, _ = pad(tags, tag_vocab[tag_vocab.PAD], device)\n",
        "            # back propagation\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = model(embded, mask, tags, sent_lengths)  # shape: (b,)\n",
        "            loss = batch_loss.mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1e5)\n",
        "            optimizer.step()\n",
        "\n",
        "            record_loss_sum += batch_loss.sum().item()\n",
        "            record_batch_size += current_batch_size\n",
        "            record_tgt_word_sum += sum(sent_lengths)\n",
        "\n",
        "            cum_loss_sum += batch_loss.sum().item()\n",
        "            cum_batch_size += current_batch_size\n",
        "            cum_tgt_word_sum += sum(sent_lengths)\n",
        "\n",
        "            if train_iter % log_every == 0:\n",
        "                print('log: epoch %d, iter %d, %.1f words/sec, avg_loss %f, time %.1f sec' %\n",
        "                      (epoch + 1, train_iter, record_tgt_word_sum / (time.time() - record_start),\n",
        "                       record_loss_sum / record_batch_size, time.time() - record_start))\n",
        "                record_loss_sum, record_batch_size, record_tgt_word_sum = 0, 0, 0\n",
        "                record_start = time.time()\n",
        "\n",
        "            if train_iter % validation_every == 0:\n",
        "                print('dev: epoch %d, iter %d, %.1f words/sec, avg_loss %f, time %.1f sec' %\n",
        "                      (epoch + 1, train_iter, cum_tgt_word_sum / (time.time() - cum_start),\n",
        "                       cum_loss_sum / cum_batch_size, time.time() - cum_start))\n",
        "                cum_loss_sum, cum_batch_size, cum_tgt_word_sum = 0, 0, 0\n",
        "\n",
        "                dev_loss = cal_dev_loss(model, dev_data, 64, sent_vocab, tag_vocab, device)\n",
        "                if dev_loss < min_dev_loss * 0.95:\n",
        "                    min_dev_loss = dev_loss\n",
        "                    model.save(model_save_path)\n",
        "                    torch.save(optimizer.state_dict(), optimizer_save_path)\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience == 2:\n",
        "                        decay_num += 1\n",
        "                        if decay_num == 2:\n",
        "                            print('Early stop. Save result model to %s' % model_save_path)\n",
        "                            return\n",
        "                        lr = optimizer.param_groups[0]['lr'] * 0.95\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                        patience = 0\n",
        "                print('dev: epoch %d, iter %d, dev_loss %f, patience %d, decay_num %d' %\n",
        "                      (epoch + 1, train_iter, dev_loss, patience, decay_num))\n",
        "                cum_start = time.time()\n",
        "                if train_iter % log_every == 0:\n",
        "                    record_start = time.time()\n",
        "    print('Reached %d epochs, Save result model to %s' % (max_epoch, model_save_path))\n",
        "\n",
        "\n",
        "def cal_dev_loss(model, dev_data, batch_size, sent_vocab, tag_vocab, device):\n",
        "    \"\"\" Calculate loss on the development data\n",
        "    Args:\n",
        "        model: the model being trained\n",
        "        dev_data: development data\n",
        "        batch_size: batch size\n",
        "        sent_vocab: sentence vocab\n",
        "        tag_vocab: tag vocab\n",
        "        device: torch.device on which the model is trained\n",
        "    Returns:\n",
        "        the average loss on the dev data\n",
        "    \"\"\"\n",
        "    is_training = model.training\n",
        "    model.eval()\n",
        "    loss, n_sentences = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in batch_iter(dev_data, batch_size, shuffle=False):\n",
        "            sentences_text = [tokenizer.convert_tokens_to_string(sent[1:-1]) for sent in sentences]\n",
        "            tokenized = tokenizer(sentences_text, return_tensors='pt',\n",
        "                                  padding=True).to(device)\n",
        "            embded = rubert_model(**tokenized)['last_hidden_state'].to(device)\n",
        "            mask = tokenized['attention_mask'].to(device)\n",
        "            sent_lengths = mask.sum(1)\n",
        "\n",
        "            tags, _ = pad(tags, tag_vocab[sent_vocab.PAD], device)\n",
        "            batch_loss = model(embded, mask, tags, sent_lengths)  # shape: (b,)\n",
        "            loss += batch_loss.sum().item()\n",
        "            n_sentences += len(sentences)\n",
        "    model.train(is_training)\n",
        "    return loss / n_sentences"
      ],
      "metadata": {
        "id": "EZF3sgm-Qs_X"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train()"
      ],
      "metadata": {
        "id": "0a6LMdw-RCQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTMCRF.load('model.pth', 'cuda')"
      ],
      "metadata": {
        "id": "lP4TkIITTYvn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_vocab = Vocab.load('sent_vocab.json')\n",
        "tag_vocab = Vocab.load('tag_vocab.json')"
      ],
      "metadata": {
        "id": "64A09rE6VdDN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Не рекомендуем сие заведение от слова совсем. Позвонили забронировать столик. Нам сказали -да на 23:00 вечера столик за Вами. Приезжаем в предвкушении повеселиться. Охранник не пускает у него нет информации что столик забронирован. Более того что у него даже не было попытки прояснить ситуацию элементарно вызвав администратора. Это всё свидетельствует о странности сотрудников сея заведения.'"
      ],
      "metadata": {
        "id": "T8wK8DTOKFny"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer([text], return_tensors='pt',\n",
        "                      padding=True).to(device)\n",
        "embded = rubert_model(**tokenized)['last_hidden_state'].to(device)\n",
        "mask = tokenized['attention_mask'].to(device)\n",
        "sent_lengths = mask.sum(1)\n",
        "tokens = model.predict(embded, mask, sent_lengths)"
      ],
      "metadata": {
        "id": "dng-LeqMWBS-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0])\n",
        "tags = [tag_vocab.id2word(i) for i in tokens[0]]"
      ],
      "metadata": {
        "id": "X8yMbWH5dCOQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = []\n",
        "new_text = text\n",
        "cur = 0\n",
        "is_span = False\n",
        "tag = ''\n",
        "for word, tag in list(zip(words, tags))[1:]:\n",
        "  new_word = word.lstrip('##')\n",
        "  start = new_text.find(new_word)\n",
        "  end = start + len(new_word)\n",
        "  if is_span and (tag.split('-')[0] != 'I' or tag.split('-')[-1] != category):\n",
        "    out.append((text[beginning:cur+1], beginning, cur+1, category))\n",
        "    is_span = False\n",
        "\n",
        "  if tag.split('-')[0] == 'B':\n",
        "    beginning = cur + start\n",
        "    category = tag.split('-')[1]\n",
        "    is_span = True\n",
        "\n",
        "  cur += end\n",
        "  new_text = new_text[end:]"
      ],
      "metadata": {
        "id": "oOb6E26IcqBo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "id": "pp6sK1sUYiIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1468b0a5-ca38-4674-f9a9-e1c27f45b014"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('заведение ', 19, 29, 'Whole'),\n",
              " ('Позвонили забронировать столик.', 46, 77, 'Service'),\n",
              " ('Охранник ', 165, 174, 'Service'),\n",
              " ('сотрудников сея заведения.', 366, 392, 'Whole')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_ZkA4OPh5r5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}